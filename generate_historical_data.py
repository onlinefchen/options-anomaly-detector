#!/usr/bin/env python3
"""
å†å²æ•°æ®ç”Ÿæˆå·¥å…·
ç”¨äºç”ŸæˆæŒ‡å®šæ—¥æœŸæˆ–æ—¥æœŸåŒºé—´çš„å†å²æ•°æ®ï¼ˆä»çœŸå®CSVæ–‡ä»¶ï¼‰
"""
import os
import sys
import json
import argparse
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from hybrid_fetcher import HybridDataFetcher
from anomaly_detector import OptionsAnomalyDetector
from report_generator import HTMLReportGenerator
from history_analyzer import HistoryAnalyzer
from archive_index_generator import get_archived_reports, generate_archive_index

# Load environment variables
load_dotenv()


def get_trading_days_in_range(start_date: str, end_date: str) -> list:
    """
    è·å–æ—¥æœŸåŒºé—´å†…çš„æ‰€æœ‰äº¤æ˜“æ—¥ï¼ˆæ’é™¤å‘¨æœ«ï¼‰

    Args:
        start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD

    Returns:
        äº¤æ˜“æ—¥æœŸåˆ—è¡¨
    """
    start = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')

    trading_days = []
    current = start

    while current <= end:
        # æ’é™¤å‘¨æœ« (0=Monday, 5=Saturday, 6=Sunday)
        if current.weekday() < 5:
            trading_days.append(current.strftime('%Y-%m-%d'))
        current += timedelta(days=1)

    return trading_days


def generate_data_for_date(date: str, output_dir: str = 'output') -> tuple:
    """
    ä¸ºæŒ‡å®šæ—¥æœŸç”Ÿæˆæ•°æ®ï¼ˆä»çœŸå®CSVæ–‡ä»¶ï¼‰

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² YYYY-MM-DD
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        (data, anomalies, summary, metadata) å¦‚æœæˆåŠŸä¸‹è½½CSV
        None å¦‚æœCSVä¸å­˜åœ¨ï¼ˆè·³è¿‡è¯¥æ—¥æœŸï¼‰
    """
    try:
        # Initialize fetcher
        fetcher = HybridDataFetcher()

        # Try to fetch data for the specified date using CSV strategy
        data, metadata = fetcher.fetch_data_for_date(date, strategy='csv', top_n_for_oi=30)

        # Check if we got CSV data
        data_source = metadata.get('data_source', 'Unknown')
        if data_source not in ['CSV', 'CSV+API']:
            print(f'  âŠ˜ No CSV available for {date} (data source: {data_source}), skipping...')
            return None

        if not data:
            print(f'  âŠ˜ No data available for {date}, skipping...')
            return None

        print(f'  âœ“ Downloaded CSV data: {len(data)} tickers')
        print(f'     Data source: {data_source}')

        # Analyze historical activity
        analyzer = HistoryAnalyzer(output_dir=output_dir, lookback_days=10)
        data = analyzer.enrich_data_with_history(data)
        print(f'  âœ“ Historical analysis complete')

        # Detect anomalies
        detector = OptionsAnomalyDetector()
        anomalies = detector.detect_all_anomalies(data)
        summary = detector.get_summary()
        print(f'  âœ“ Detected {summary["total"]} anomalies')

        return data, anomalies, summary, metadata

    except Exception as e:
        print(f'  âŒ Error processing {date}: {e}')
        import traceback
        traceback.print_exc()
        return None


def save_historical_data(date: str, data: list, anomalies: list, summary: dict,
                         metadata: dict, output_dir: str = 'output'):
    """
    ä¿å­˜å†å²æ•°æ®åˆ°æ–‡ä»¶

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²
        data: æ•°æ®åˆ—è¡¨
        anomalies: å¼‚å¸¸åˆ—è¡¨
        summary: ç»Ÿè®¡æ‘˜è¦
        metadata: å…ƒæ•°æ®ï¼ˆåŒ…å«data_sourceç­‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜ JSON
    data_source = metadata.get('data_source', 'Unknown')
    historical_data = {
        'date': date,
        'timestamp': datetime.now().isoformat(),
        'tickers_count': len(data),
        'anomalies_count': summary.get('total', 0),
        'data_source': data_source,
        'data': data,
        'anomalies': anomalies,
        'summary': summary
    }

    json_file = os.path.join(output_dir, f'{date}.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(historical_data, f, ensure_ascii=False, indent=2)

    print(f'  âœ“ JSON saved: {json_file}')

    # ç”Ÿæˆ HTML æŠ¥å‘Š
    reporter = HTMLReportGenerator()
    html_file = os.path.join(output_dir, f'{date}.html')
    reporter.generate(
        data=data,
        anomalies=anomalies,
        summary=summary,
        metadata=metadata,
        output_file=html_file
    )

    print(f'  âœ“ HTML saved: {html_file}')


def main():
    parser = argparse.ArgumentParser(
        description='ç”ŸæˆæŒ‡å®šæ—¥æœŸæˆ–æ—¥æœŸåŒºé—´çš„å†å²æ•°æ®ï¼ˆä»çœŸå®CSVæ–‡ä»¶ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ç”Ÿæˆå•ä¸ªæ—¥æœŸçš„æ•°æ®
  python generate_historical_data.py --date 2025-10-20

  # ç”Ÿæˆæ—¥æœŸåŒºé—´çš„æ•°æ®
  python generate_historical_data.py --start 2025-10-20 --end 2025-10-29

  # ç”Ÿæˆè¿‡å»10ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
  python generate_historical_data.py --days 10

æ³¨æ„:
  - éœ€è¦é…ç½® POLYGON_S3_ACCESS_KEY å’Œ POLYGON_S3_SECRET_KEY
  - åªä¼šä¸‹è½½å­˜åœ¨CSVæ–‡ä»¶çš„æ—¥æœŸï¼Œä¸å­˜åœ¨çš„æ—¥æœŸä¼šè‡ªåŠ¨è·³è¿‡
  - å‘¨æœ«å’ŒèŠ‚å‡æ—¥é€šå¸¸æ²¡æœ‰CSVæ–‡ä»¶
        """
    )

    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--date', help='å•ä¸ªæ—¥æœŸ (YYYY-MM-DD)')
    group.add_argument('--start', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œéœ€é…åˆ --end ä½¿ç”¨')
    group.add_argument('--days', type=int, help='ç”Ÿæˆè¿‡å»Nä¸ªäº¤æ˜“æ—¥çš„æ•°æ®')

    parser.add_argument('--end', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œé…åˆ --start ä½¿ç”¨')
    parser.add_argument('--output', default='output', help='è¾“å‡ºç›®å½• (é»˜è®¤: output)')

    args = parser.parse_args()

    print("=" * 70)
    print("å†å²æ•°æ®ç”Ÿæˆå·¥å…· (ä»çœŸå®CSVæ–‡ä»¶)")
    print("=" * 70)
    print()

    # ç¡®å®šè¦ç”Ÿæˆçš„æ—¥æœŸåˆ—è¡¨
    dates = []

    if args.date:
        # å•ä¸ªæ—¥æœŸ
        dates = [args.date]
        print(f"æ¨¡å¼: ç”Ÿæˆå•ä¸ªæ—¥æœŸ")
        print(f"æ—¥æœŸ: {args.date}")

    elif args.start:
        # æ—¥æœŸåŒºé—´
        if not args.end:
            parser.error("ä½¿ç”¨ --start æ—¶å¿…é¡»æŒ‡å®š --end")

        dates = get_trading_days_in_range(args.start, args.end)
        print(f"æ¨¡å¼: ç”Ÿæˆæ—¥æœŸåŒºé—´")
        print(f"åŒºé—´: {args.start} è‡³ {args.end}")
        print(f"äº¤æ˜“æ—¥: {len(dates)} å¤©ï¼ˆæ’é™¤å‘¨æœ«ï¼‰")

    elif args.days:
        # è¿‡å»Nä¸ªäº¤æ˜“æ—¥
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days * 2)  # é¢„ç•™è¶³å¤Ÿçš„å¤©æ•°

        dates = get_trading_days_in_range(
            start_date.strftime('%Y-%m-%d'),
            end_date.strftime('%Y-%m-%d')
        )[-args.days:]  # å–æœ€åNä¸ªäº¤æ˜“æ—¥

        print(f"æ¨¡å¼: ç”Ÿæˆè¿‡å»Nä¸ªäº¤æ˜“æ—¥")
        print(f"å¤©æ•°: {args.days} ä¸ªäº¤æ˜“æ—¥")
        print(f"æ—¥æœŸèŒƒå›´: {dates[0]} è‡³ {dates[-1]}")

    print()
    print("=" * 70)
    print(f"å¼€å§‹ä¸‹è½½ {len(dates)} å¤©çš„CSVæ•°æ®...")
    print("=" * 70)
    print()

    # ç”Ÿæˆæ•°æ®
    today = datetime.now()
    success_count = 0
    skip_count = 0

    for date in dates:
        date_obj = datetime.strptime(date, '%Y-%m-%d')
        days_ago = (today - date_obj).days

        print(f"å¤„ç† {date} (è·ä»Š {days_ago} å¤©)...")

        result = generate_data_for_date(date, args.output)

        if result is None:
            skip_count += 1
            print(f'  âŠ˜ è·³è¿‡ {date}')
        else:
            data, anomalies, summary, metadata = result
            save_historical_data(date, data, anomalies, summary, metadata, args.output)
            success_count += 1
            print(f'  âœ“ å®Œæˆ {date}')

        print()

    print("=" * 70)
    print(f"âœ… å®Œæˆï¼")
    print("=" * 70)
    print(f"  â€¢ æˆåŠŸ: {success_count} å¤©")
    print(f"  â€¢ è·³è¿‡: {skip_count} å¤©ï¼ˆæ— CSVï¼‰")
    print()
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - {args.output}/*.json  (åŸå§‹æ•°æ®)")
    print(f"  - {args.output}/*.html  (HTMLæŠ¥å‘Š)")
    print()

    # Generate archive index if we have any reports
    if success_count > 0:
        print("ğŸ“š ç”Ÿæˆå½’æ¡£ç´¢å¼•...")
        reports = get_archived_reports(args.output)
        generate_archive_index(reports, args.output)
        print(f"âœ“ å½’æ¡£ç´¢å¼•æ›´æ–°å®Œæˆ ({len(reports)} ä¸ªæŠ¥å‘Š)")
        print()

    print("ä¸‹ä¸€æ­¥:")
    print("  1. è¿è¡Œ main.py è¿›è¡Œä¸€æ¬¡å®Œæ•´åˆ†æ")
    print("  2. æŸ¥çœ‹æŠ¥å‘Šä¸­çš„ '10æ—¥æ´»è·ƒåº¦' åˆ—")
    print("  3. åº”è¯¥èƒ½çœ‹åˆ°å®Œæ•´çš„å†å²ç»Ÿè®¡æ•°æ®")
    print()


if __name__ == '__main__':
    main()
