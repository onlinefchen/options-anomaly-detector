#!/usr/bin/env python3
"""
å†å²æ•°æ®ç”Ÿæˆå·¥å…·
ç”¨äºç”ŸæˆæŒ‡å®šæ—¥æœŸæˆ–æ—¥æœŸåŒºé—´çš„å†å²æ•°æ®ï¼ˆä»çœŸå®CSVæ–‡ä»¶ï¼‰
"""
import os
import sys
import json
import argparse
from datetime import datetime, timedelta
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from hybrid_fetcher import HybridDataFetcher
from anomaly_detector import OptionsAnomalyDetector
from report_generator import HTMLReportGenerator
from history_analyzer import HistoryAnalyzer
from archive_index_generator import get_archived_reports, generate_archive_index
from trading_calendar import has_trading_days_between, get_previous_trading_day, get_trading_calendar

# Load environment variables
load_dotenv()


def get_trading_days_in_range(start_date: str, end_date: str) -> list:
    """
    è·å–æ—¥æœŸåŒºé—´å†…çš„æ‰€æœ‰äº¤æ˜“æ—¥ï¼ˆä½¿ç”¨ NYSE äº¤æ˜“æ—¥å†ï¼‰

    Args:
        start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD

    Returns:
        äº¤æ˜“æ—¥æœŸåˆ—è¡¨
    """
    calendar = get_trading_calendar()
    return calendar.get_trading_days_in_range(start_date, end_date)


def generate_data_for_date(date: str, output_dir: str = 'output') -> tuple:
    """
    ä¸ºæŒ‡å®šæ—¥æœŸç”Ÿæˆæ•°æ®ï¼ˆä»çœŸå®CSVæ–‡ä»¶ï¼‰

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸² YYYY-MM-DD
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        (data, anomalies, summary, metadata) å¦‚æœæˆåŠŸä¸‹è½½CSV
        None å¦‚æœCSVä¸å­˜åœ¨ï¼ˆè·³è¿‡è¯¥æ—¥æœŸï¼‰
    """
    try:
        print(f'ğŸ“¥ STEP 1/4: ä¸‹è½½ CSV æ–‡ä»¶')
        print(f'   ç›®æ ‡æ—¥æœŸ: {date}')

        # Initialize fetcher to get CSV handler
        fetcher = HybridDataFetcher()

        # Try to download and parse CSV for the specified date
        print(f'   â³ æ­£åœ¨å°è¯•ä¸‹è½½ {date}.csv.gz ...')
        success, data, csv_date = fetcher.csv_handler.try_download_and_parse(date=date, max_retries=1)

        if not success or not data:
            print(f'   âŒ CSVä¸‹è½½å¤±è´¥ - æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®')
            print(f'   âŠ˜ è·³è¿‡ {date}ï¼Œä¸ç”Ÿæˆä»»ä½•æ–‡ä»¶')
            return None

        print(f'   âœ… CSVä¸‹è½½æˆåŠŸï¼')
        print(f'      - æ–‡ä»¶: {csv_date}.csv.gz')
        print(f'      - æ•°æ®: {len(data)} ä¸ªæ ‡çš„')
        print(f'      - æ€»æˆäº¤é‡: {sum(d["total_volume"] for d in data):,}')
        print()

        # Algorithm 2: Determine if OI should be fetched
        print(f'ğŸ“¡ STEP 2/5: æ£€æŸ¥æ˜¯å¦éœ€è¦è·å– Open Interest æ•°æ®')
        current_date = datetime.now().strftime('%Y-%m-%d')
        should_fetch_oi = not has_trading_days_between(csv_date, current_date)

        if should_fetch_oi:
            print(f'   âœ“ {csv_date} è‡³ä»Šæ— æ–°äº¤æ˜“æ—¥')
            print(f'   â†’ OI æ•°æ®æœ‰æ„ä¹‰ï¼ˆåæ˜  {csv_date} ç›˜åå¸‚åœºçŠ¶æ€ï¼‰')
            print(f'   â³ æ­£åœ¨ä¸ºå‰ 35 ä¸ªæ ‡çš„è·å– OI æ•°æ®...')
            data, metadata = fetcher.enrich_with_oi(data, top_n=35)
            print(f'   âœ… OI æ•°æ®è·å–å®Œæˆ')
        else:
            print(f'   âŠ˜ {csv_date} è‡³ä»Šæœ‰æ–°äº¤æ˜“æ—¥')
            print(f'   â†’ OI æ•°æ®æ— æ„ä¹‰ï¼ˆä¼šæ˜¯ä»Šå¤©çš„æ•°æ®ï¼Œä¸æ˜¯ {csv_date} çš„ï¼‰')
            print(f'   â†’ è·³è¿‡ OI è·å–')
            metadata = {
                'data_source': 'CSV',
                'csv_date': csv_date,
                'oi_skipped': 'historical_data',
                'oi_skip_reason': f'New trading days exist between {csv_date} and {current_date}'
            }
        print()

        print(f'ğŸ“Š STEP 3/5: åˆ†æå†å²æ´»è·ƒåº¦')
        print(f'   â³ æ­£åœ¨åˆ†æ {date} çš„å†å²æ•°æ®...')
        analyzer = HistoryAnalyzer(output_dir=output_dir, lookback_days=10)
        data = analyzer.enrich_data_with_history(data)
        print(f'   âœ… å†å²åˆ†æå®Œæˆ')
        print()

        print(f'ğŸ” STEP 4/5: æ£€æµ‹å¼‚å¸¸ä¿¡å·')
        print(f'   â³ æ­£åœ¨æ£€æµ‹ {date} çš„å¸‚åœºå¼‚å¸¸...')
        detector = OptionsAnomalyDetector()
        anomalies = detector.detect_all_anomalies(data)
        summary = detector.get_summary()
        print(f'   âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ')
        print(f'      - æ£€æµ‹åˆ° {summary["total"]} ä¸ªå¼‚å¸¸ä¿¡å·')

        # Show by type breakdown if available
        if summary.get('by_type'):
            print(f'      - æŒ‰ç±»å‹åˆ†å¸ƒ:')
            for atype, count in sorted(summary['by_type'].items(), key=lambda x: x[1], reverse=True):
                print(f'        â€¢ {atype}: {count}')

        # Show by severity breakdown if available
        if summary.get('by_severity'):
            print(f'      - æŒ‰ä¸¥é‡ç¨‹åº¦:')
            for severity, count in sorted(summary['by_severity'].items(), key=lambda x: x[1], reverse=True):
                print(f'        â€¢ {severity}: {count}')
        print()

        metadata = {
            'data_source': 'CSV',
            'csv_date': csv_date
        }

        print(f'âœ… {date} æ•°æ®å‡†å¤‡å®Œæˆï¼Œç­‰å¾…ä¿å­˜...')
        return data, anomalies, summary, metadata

    except Exception as e:
        print(f'   âŒ å¤„ç† {date} æ—¶å‘ç”Ÿé”™è¯¯: {e}')
        print(f'   âŠ˜ è·³è¿‡ {date}ï¼Œä¸ç”Ÿæˆä»»ä½•æ–‡ä»¶')
        import traceback
        traceback.print_exc()
        return None


def save_historical_data(date: str, data: list, anomalies: list, summary: dict,
                         metadata: dict, output_dir: str = 'output'):
    """
    ä¿å­˜å†å²æ•°æ®åˆ°æ–‡ä»¶

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²
        data: æ•°æ®åˆ—è¡¨
        anomalies: å¼‚å¸¸åˆ—è¡¨
        summary: ç»Ÿè®¡æ‘˜è¦
        metadata: å…ƒæ•°æ®ï¼ˆåŒ…å«data_sourceç­‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f'ğŸ’¾ STEP 5/5: ä¿å­˜æ•°æ®æ–‡ä»¶')
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜ JSON
    data_source = metadata.get('data_source', 'CSV')
    historical_data = {
        'date': date,  # CSV date (data date)
        'generated_at': datetime.now().isoformat(),  # When report was generated
        'tickers_count': len(data),
        'anomalies_count': summary.get('total', 0),
        'data_source': data_source,
        'data': data,
        'anomalies': anomalies,
        'summary': summary,
        'metadata': metadata  # Include full metadata (OI skip info, etc.)
    }

    json_file = os.path.join(output_dir, f'{date}.json')
    print(f'   â³ æ­£åœ¨ä¿å­˜ JSON: {date}.json ...')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(historical_data, f, ensure_ascii=False, indent=2)
    file_size = os.path.getsize(json_file) / 1024
    print(f'   âœ… JSON å·²ä¿å­˜: {json_file} ({file_size:.1f} KB)')

    # ç”Ÿæˆ HTML æŠ¥å‘Š
    html_file = os.path.join(output_dir, f'{date}.html')
    print(f'   â³ æ­£åœ¨ç”Ÿæˆ HTML: {date}.html ...')
    reporter = HTMLReportGenerator()
    reporter.generate(
        data=data,
        anomalies=anomalies,
        summary=summary,
        metadata=metadata,
        output_file=html_file
    )
    file_size = os.path.getsize(html_file) / 1024
    print(f'   âœ… HTML å·²ä¿å­˜: {html_file} ({file_size:.1f} KB)')
    print()


def main():
    parser = argparse.ArgumentParser(
        description='ç”ŸæˆæŒ‡å®šæ—¥æœŸæˆ–æ—¥æœŸåŒºé—´çš„å†å²æ•°æ®ï¼ˆä»çœŸå®CSVæ–‡ä»¶ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ç”Ÿæˆå•ä¸ªæ—¥æœŸçš„æ•°æ®
  python generate_historical_data.py --date 2025-10-20

  # ç”Ÿæˆæ—¥æœŸåŒºé—´çš„æ•°æ®
  python generate_historical_data.py --start 2025-10-20 --end 2025-10-29

  # ç”Ÿæˆè¿‡å»10ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
  python generate_historical_data.py --days 10

æ³¨æ„:
  - éœ€è¦é…ç½® POLYGON_S3_ACCESS_KEY å’Œ POLYGON_S3_SECRET_KEY
  - åªä¼šä¸‹è½½å­˜åœ¨CSVæ–‡ä»¶çš„æ—¥æœŸï¼Œä¸å­˜åœ¨çš„æ—¥æœŸä¼šè‡ªåŠ¨è·³è¿‡
  - å‘¨æœ«å’ŒèŠ‚å‡æ—¥é€šå¸¸æ²¡æœ‰CSVæ–‡ä»¶
        """
    )

    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--date', help='å•ä¸ªæ—¥æœŸ (YYYY-MM-DD)')
    group.add_argument('--start', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œéœ€é…åˆ --end ä½¿ç”¨')
    group.add_argument('--days', type=int, help='ç”Ÿæˆè¿‡å»Nä¸ªäº¤æ˜“æ—¥çš„æ•°æ®')

    parser.add_argument('--end', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œé…åˆ --start ä½¿ç”¨')
    parser.add_argument('--output', default='output', help='è¾“å‡ºç›®å½• (é»˜è®¤: output)')

    args = parser.parse_args()

    print("=" * 70)
    print("å†å²æ•°æ®ç”Ÿæˆå·¥å…· (ä»çœŸå®CSVæ–‡ä»¶)")
    print("=" * 70)
    print()

    # ç¡®å®šè¦ç”Ÿæˆçš„æ—¥æœŸåˆ—è¡¨
    dates = []

    if args.date:
        # å•ä¸ªæ—¥æœŸ
        dates = [args.date]
        print(f"æ¨¡å¼: ç”Ÿæˆå•ä¸ªæ—¥æœŸ")
        print(f"æ—¥æœŸ: {args.date}")

    elif args.start:
        # æ—¥æœŸåŒºé—´
        if not args.end:
            parser.error("ä½¿ç”¨ --start æ—¶å¿…é¡»æŒ‡å®š --end")

        dates = get_trading_days_in_range(args.start, args.end)
        print(f"æ¨¡å¼: ç”Ÿæˆæ—¥æœŸåŒºé—´")
        print(f"åŒºé—´: {args.start} è‡³ {args.end}")
        print(f"äº¤æ˜“æ—¥: {len(dates)} å¤©ï¼ˆæ’é™¤å‘¨æœ«ï¼‰")

    elif args.days:
        # è¿‡å»Nä¸ªäº¤æ˜“æ—¥
        # ä½¿ç”¨å‰ä¸€ä¸ªå·²å®Œæˆçš„äº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸï¼ˆä¸åŒ…å«ä»Šå¤©ï¼‰
        end_date_str = get_previous_trading_day()
        end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
        start_date = end_date - timedelta(days=args.days * 2)  # é¢„ç•™è¶³å¤Ÿçš„å¤©æ•°

        dates = get_trading_days_in_range(
            start_date.strftime('%Y-%m-%d'),
            end_date.strftime('%Y-%m-%d')
        )[-args.days:]  # å–æœ€åNä¸ªäº¤æ˜“æ—¥

        print(f"æ¨¡å¼: ç”Ÿæˆè¿‡å»Nä¸ªäº¤æ˜“æ—¥")
        print(f"å¤©æ•°: {args.days} ä¸ªäº¤æ˜“æ—¥")
        print(f"æ—¥æœŸèŒƒå›´: {dates[0]} è‡³ {dates[-1]}")
        print(f"æ³¨æ„: ç»“æŸæ—¥æœŸæ˜¯æœ€åä¸€ä¸ªå·²å®Œæˆçš„äº¤æ˜“æ—¥ ({end_date_str})")

    print()
    print("=" * 70)
    print(f"å¼€å§‹ä¸‹è½½ {len(dates)} å¤©çš„CSVæ•°æ®...")
    print("=" * 70)
    print()

    # ç”Ÿæˆæ•°æ®
    today = datetime.now()
    success_count = 0
    skip_count = 0
    total_days = len(dates)

    for idx, date in enumerate(dates, 1):
        date_obj = datetime.strptime(date, '%Y-%m-%d')
        days_ago = (today - date_obj).days

        print("â”" * 70)
        print(f"ğŸ“… [{idx}/{total_days}] å¤„ç†æ—¥æœŸ: {date} (è·ä»Š {days_ago} å¤©)")
        print(f"   è¿›åº¦: {idx}/{total_days} ({idx*100//total_days}%) | æˆåŠŸ: {success_count} | è·³è¿‡: {skip_count}")
        print("â”" * 70)

        result = generate_data_for_date(date, args.output)

        if result is None:
            skip_count += 1
            print(f'â”' * 70)
            print(f'âŒ {date} å¤„ç†å¤±è´¥ - CSVæ–‡ä»¶ä¸å¯ç”¨ï¼Œå·²è·³è¿‡')
            print(f'â”' * 70)
        else:
            data, anomalies, summary, metadata = result
            save_historical_data(date, data, anomalies, summary, metadata, args.output)
            success_count += 1
            print(f'â”' * 70)
            print(f'âœ… {date} å¤„ç†å®Œæˆï¼')
            print(f'â”' * 70)

        print(f'ğŸ“Š æ±‡æ€»ç»Ÿè®¡: å·²å®Œæˆ {idx}/{total_days} | æˆåŠŸ {success_count} | è·³è¿‡ {skip_count} | å‰©ä½™ {total_days - idx}')
        print()
        print()

    print("=" * 70)
    print(f"âœ… å®Œæˆï¼")
    print("=" * 70)
    print(f"  â€¢ æˆåŠŸ: {success_count} å¤©")
    print(f"  â€¢ è·³è¿‡: {skip_count} å¤©ï¼ˆæ— CSVï¼‰")
    print()
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - {args.output}/*.json  (åŸå§‹æ•°æ®)")
    print(f"  - {args.output}/*.html  (HTMLæŠ¥å‘Š)")
    print()

    # Generate archive index if we have any reports
    if success_count > 0:
        print("ğŸ“š ç”Ÿæˆå½’æ¡£ç´¢å¼•...")
        reports = get_archived_reports(args.output)
        generate_archive_index(reports, os.path.join(args.output, 'archive.html'))
        print(f"âœ“ å½’æ¡£ç´¢å¼•æ›´æ–°å®Œæˆ ({len(reports)} ä¸ªæŠ¥å‘Š)")
        print()

    print("ä¸‹ä¸€æ­¥:")
    print("  1. è¿è¡Œ main.py è¿›è¡Œä¸€æ¬¡å®Œæ•´åˆ†æ")
    print("  2. æŸ¥çœ‹æŠ¥å‘Šä¸­çš„ '10æ—¥æ´»è·ƒåº¦' åˆ—")
    print("  3. åº”è¯¥èƒ½çœ‹åˆ°å®Œæ•´çš„å†å²ç»Ÿè®¡æ•°æ®")
    print()


if __name__ == '__main__':
    main()
