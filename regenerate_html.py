#!/usr/bin/env python3
"""
é‡æ–°ç”Ÿæˆæ‰€æœ‰HTMLæŠ¥å‘Šï¼ˆä»ç°æœ‰JSONæ•°æ®ï¼‰
"""
import os
import sys
import json
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from report_generator import HTMLReportGenerator
from archive_index_generator import get_archived_reports, generate_archive_index

load_dotenv()


def regenerate_html_from_json(json_file: str):
    """
    ä»JSONæ–‡ä»¶é‡æ–°ç”ŸæˆHTML

    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
    """
    print(f'Processing {json_file}...')

    try:
        # Load JSON data
        with open(json_file, 'r', encoding='utf-8') as f:
            historical_data = json.load(f)

        date = historical_data.get('date')
        data = historical_data.get('data', [])
        anomalies = historical_data.get('anomalies', [])
        summary = historical_data.get('summary', {})
        metadata = historical_data.get('metadata', {})

        # Generate HTML
        html_file = json_file.replace('.json', '.html')
        reporter = HTMLReportGenerator()
        reporter.generate(
            data=data,
            anomalies=anomalies,
            summary=summary,
            metadata=metadata,
            output_file=html_file
        )

        print(f'  âœ“ Generated {html_file}')
        return True

    except Exception as e:
        print(f'  âœ— Error: {e}')
        return False


def main():
    """Main execution"""
    print("=" * 70)
    print("é‡æ–°ç”ŸæˆHTMLæŠ¥å‘Šï¼ˆä»ç°æœ‰JSONæ•°æ®ï¼‰")
    print("=" * 70)
    print()

    output_dir = 'output'

    # Find all JSON files
    json_files = []
    for filename in os.listdir(output_dir):
        if filename.endswith('.json') and len(filename) == 15:  # YYYY-MM-DD.json
            json_files.append(os.path.join(output_dir, filename))

    json_files.sort()

    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
    print()

    # Regenerate HTML for each JSON
    success_count = 0
    for json_file in json_files:
        if regenerate_html_from_json(json_file):
            success_count += 1

    print()
    print("=" * 70)
    print(f"å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/{len(json_files)} ä¸ªHTMLæ–‡ä»¶")
    print("=" * 70)
    print()

    # Regenerate archive index
    print("ğŸ“š é‡æ–°ç”Ÿæˆå½’æ¡£ç´¢å¼•...")
    reports = get_archived_reports(output_dir)
    generate_archive_index(reports, os.path.join(output_dir, 'archive.html'))
    print(f"âœ“ å½’æ¡£ç´¢å¼•æ›´æ–°å®Œæˆ ({len(reports)} ä¸ªæŠ¥å‘Š)")
    print()

    # Copy latest report to index.html
    if json_files:
        latest_date = max([f.split('/')[-1].replace('.json', '') for f in json_files])
        latest_html = os.path.join(output_dir, f'{latest_date}.html')
        index_html = os.path.join(output_dir, 'index.html')

        if os.path.exists(latest_html):
            import shutil
            shutil.copy2(latest_html, index_html)
            print(f"âœ“ å¤åˆ¶æœ€æ–°æŠ¥å‘Šåˆ° index.html ({latest_date})")

    print()
    print("å®Œæˆï¼")


if __name__ == '__main__':
    main()
