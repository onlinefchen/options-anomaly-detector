name: Daily Options Analysis

on:
  # CSV-CENTRIC WORKFLOW: Run daily at 4:00 PM Beijing Time
  # Purpose: Check if Polygon has uploaded today's CSV file and process it
  # CSV is THE CORE prerequisite - without it, analysis cannot proceed
  schedule:
    - cron: '0 8 * * *'  # 08:00 UTC = 4:00 PM Beijing Time (daily)
                         # Runs once per day to check and process CSV

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      force_overwrite:
        description: 'üîÑ Âº∫Âà∂ÈáçÊñ∞ÁîüÊàê - Âç≥‰ΩøÊï∞ÊçÆÂ∑≤Â≠òÂú®‰πüÈáçÊñ∞‰∏ãËΩΩ„ÄÅÂàÜÊûê„ÄÅÂèëÈÇÆ‰ª∂'
        required: false
        type: boolean
        default: false
      days_back:
        description: 'üìä ÁîüÊàêËøáÂéªN‰∏™‰∫§ÊòìÊó•ÁöÑÊï∞ÊçÆÔºàÁïôÁ©∫ÂàôÂè™ÁîüÊàê‰ªäÂ§©Ôºâ'
        required: false
        type: number
        default: 0

  # Run on push to main for testing
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'main.py'
      - '.github/workflows/**'

jobs:
  # Job 1: Êï∞ÊçÆ‰∏ãËΩΩ‰∏éÂàÜÊûê
  fetch-and-analyze:
    name: üìä Data Fetching & Analysis
    runs-on: ubuntu-latest

    outputs:
      analysis-date: ${{ steps.get-date.outputs.date }}

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for historical analysis

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: üì¶ Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: üìÖ Get current date
        id: get-date
        run: echo "date=$(TZ='Asia/Shanghai' date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

      - name: üìÇ Restore historical data
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: gh-pages-data
        continue-on-error: true

      - name: üíæ Copy historical JSON files
        run: |
          mkdir -p output
          if [ -d "gh-pages-data" ]; then
            echo "üìä Copying historical JSON files for 10-day analysis..."
            find gh-pages-data -name "*.json" -type f -exec cp {} output/ \; || true
            echo "‚úì Historical data restored: $(ls output/*.json 2>/dev/null | wc -l) files"
          else
            echo "‚ö†Ô∏è  No historical data found (first run?)"
          fi

      - name: üìä Generate historical data (if days_back specified)
        if: ${{ github.event.inputs.days_back != '' && github.event.inputs.days_back != '0' }}
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          POLYGON_S3_ACCESS_KEY: ${{ secrets.POLYGON_S3_ACCESS_KEY }}
          POLYGON_S3_SECRET_KEY: ${{ secrets.POLYGON_S3_SECRET_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASSWD: ${{ secrets.GMAIL_APP_PASSWD }}
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENT_EMAIL }}
          FORCE_OVERWRITE: ${{ github.event.inputs.force_overwrite }}
        run: |
          echo "=================================================="
          echo "üìä GENERATING PAST ${{ github.event.inputs.days_back }} TRADING DAYS"
          echo "=================================================="
          echo ""
          echo "Force overwrite: ${{ github.event.inputs.force_overwrite }}"
          echo ""
          python generate_historical_data.py --days ${{ github.event.inputs.days_back }}
          echo ""
          echo "=================================================="

      - name: üîç Run daily analysis (if days_back not specified)
        if: ${{ github.event.inputs.days_back == '' || github.event.inputs.days_back == '0' }}
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          POLYGON_S3_ACCESS_KEY: ${{ secrets.POLYGON_S3_ACCESS_KEY }}
          POLYGON_S3_SECRET_KEY: ${{ secrets.POLYGON_S3_SECRET_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          FORCE_OVERWRITE: ${{ github.event.inputs.force_overwrite }}
        run: |
          echo "=================================================="
          echo "üîç STEP 2: ANOMALY DETECTION & ANALYSIS"
          echo "=================================================="
          python main.py

      - name: üîß Fix missing HTML files
        run: |
          echo "=================================================="
          echo "üîß FIXING MISSING HTML FILES"
          echo "=================================================="
          python scripts/regenerate_html_from_json.py --all-missing
          echo ""

      - name: üìö Generate archive index (always update)
        run: |
          echo "=================================================="
          echo "üìö GENERATING ARCHIVE INDEX"
          echo "=================================================="
          python3 -c "
          import sys
          sys.path.insert(0, 'src')
          from archive_index_generator import get_archived_reports, generate_archive_index
          reports = get_archived_reports('output')
          generate_archive_index(reports, 'output/archive.html')
          print(f'‚úì Archive index generated: {len(reports)} reports')
          "
          echo ""

      - name: üìä Upload analysis results
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results-${{ steps.get-date.outputs.date }}
          path: |
            output/
            !output/.gitkeep
          retention-days: 90

      - name: ü§ñ AI Analysis & Email Notification
        if: success() && github.event_name != 'push'
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASSWD: ${{ secrets.GMAIL_APP_PASSWD }}
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENT_EMAIL }}
        run: |
          echo "=================================================="
          echo "ü§ñ AI ANALYSIS & EMAIL NOTIFICATION"
          echo "=================================================="
          python scripts/run_ai_email.py


  # Job 2: ÁΩëÈ°µÈÉ®ÁΩ≤
  deploy:
    name: üöÄ Deploy to GitHub Pages
    needs: fetch-and-analyze
    if: success()
    runs-on: ubuntu-latest

    permissions:
      contents: write  # Needed for pushing to gh-pages

    steps:
      - name: üì• Download analysis results
        uses: actions/download-artifact@v4
        with:
          name: analysis-results-${{ needs.fetch-and-analyze.outputs.analysis-date }}
          path: output

      - name: üöÄ Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./output
          publish_branch: gh-pages
          keep_files: true  # Keep historical data files
          commit_message: 'üìä Update report - ${{ needs.fetch-and-analyze.outputs.analysis-date }}'
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          enable_jekyll: false

      - name: ‚úÖ Deployment summary
        run: |
          echo "=================================================="
          echo "‚úÖ DEPLOYMENT COMPLETE"
          echo "=================================================="
          echo "üìÖ Analysis Date: ${{ needs.fetch-and-analyze.outputs.analysis-date }}"
          echo "üîó Report URL: https://onlinefchen.github.io/options-anomaly-detector/"
          echo "üì¶ Files deployed: $(ls output | wc -l)"
          echo "=================================================="
