name: Daily Options Analysis

on:
  # Run daily at 6:00 AM Beijing Time (with AI analysis and email notification)
  schedule:
    - cron: '0 22 * * 0-4'  # 22:00 UTC = 6:00 AM Beijing Time next day (Sun-Thu UTC = Mon-Fri Beijing)

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      force_generate:
        description: '🔄 强制生成历史数据'
        required: false
        type: boolean
        default: false
      start_date:
        description: '📅 开始日期 (YYYY-MM-DD)'
        required: false
        type: string
        default: ''
      end_date:
        description: '📅 结束日期 (YYYY-MM-DD，留空则只生成开始日期)'
        required: false
        type: string
        default: ''
      days_back:
        description: '📊 或者：生成过去N个交易日（优先级最高）'
        required: false
        type: number
        default: 0

  # Run on push to main for testing
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'main.py'
      - '.github/workflows/**'

jobs:
  # Job 1: 数据下载与分析
  fetch-and-analyze:
    name: 📊 Data Fetching & Analysis
    runs-on: ubuntu-latest

    outputs:
      analysis-date: ${{ steps.get-date.outputs.date }}

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for historical analysis

      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: 📦 Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: 📅 Get current date
        id: get-date
        run: echo "date=$(TZ='Asia/Shanghai' date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

      - name: 📂 Restore historical data
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: gh-pages-data
        continue-on-error: true

      - name: 💾 Copy historical JSON files
        run: |
          mkdir -p output
          if [ -d "gh-pages-data" ]; then
            echo "📊 Copying historical JSON files for 10-day analysis..."
            find gh-pages-data -name "*.json" -type f -exec cp {} output/ \; || true
            echo "✓ Historical data restored: $(ls output/*.json 2>/dev/null | wc -l) files"
          else
            echo "⚠️  No historical data found (first run?)"
          fi

      - name: 📡 Fetch options data
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          echo "=================================================="
          echo "📡 STEP 1: DATA FETCHING"
          echo "=================================================="
          echo "Date: $(TZ='Asia/Shanghai' date +'%Y-%m-%d %H:%M:%S %Z')"
          echo "Market Session: Will be detected by script"
          echo ""

      - name: 📜 Generate historical data (if requested)
        if: ${{ github.event.inputs.force_generate == 'true' }}
        run: |
          echo "=================================================="
          echo "📜 GENERATING HISTORICAL DATA"
          echo "=================================================="
          echo ""

          # 判断使用哪种模式
          if [ "${{ github.event.inputs.days_back }}" != "0" ] && [ "${{ github.event.inputs.days_back }}" != "" ]; then
            echo "模式: 生成过去 ${{ github.event.inputs.days_back }} 个交易日"
            python generate_historical_data.py --days ${{ github.event.inputs.days_back }}
          elif [ -n "${{ github.event.inputs.start_date }}" ]; then
            if [ -n "${{ github.event.inputs.end_date }}" ]; then
              echo "模式: 生成日期区间 ${{ github.event.inputs.start_date }} 至 ${{ github.event.inputs.end_date }}"
              python generate_historical_data.py --start "${{ github.event.inputs.start_date }}" --end "${{ github.event.inputs.end_date }}"
            else
              echo "模式: 生成单个日期 ${{ github.event.inputs.start_date }}"
              python generate_historical_data.py --date "${{ github.event.inputs.start_date }}"
            fi
          else
            echo "❌ 错误: 必须指定 days_back 或 start_date"
            exit 1
          fi

          echo ""
          echo "✓ 历史数据生成完成"
          echo ""

      - name: 🔍 Run anomaly detection
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          echo "=================================================="
          echo "🔍 STEP 2: ANOMALY DETECTION & ANALYSIS"
          echo "=================================================="
          python main.py

      - name: 📊 Upload analysis results
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results-${{ steps.get-date.outputs.date }}
          path: |
            output/
            !output/.gitkeep
          retention-days: 90

      - name: 🤖 AI Analysis & Email Notification
        if: github.event_name != 'push'
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASSWD: ${{ secrets.GMAIL_APP_PASSWD }}
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENT_EMAIL }}
        run: |
          echo "=================================================="
          echo "🤖 AI ANALYSIS & EMAIL NOTIFICATION"
          echo "=================================================="
          echo ""

          # 检查是否配置了 AI 或邮件
          HAS_AI=false
          HAS_EMAIL=false

          if [ -n "$OPENAI_API_KEY" ]; then
            echo "✓ OpenAI API Key configured"
            HAS_AI=true
          else
            echo "⊘ OpenAI API Key not configured (skipping AI analysis)"
          fi

          if [ -n "$GMAIL_USER" ] && [ -n "$GMAIL_APP_PASSWD" ]; then
            echo "✓ Gmail credentials configured"
            HAS_EMAIL=true
          else
            echo "⊘ Gmail not configured (skipping email)"
          fi

          echo ""

          # 如果两者都未配置，跳过
          if [ "$HAS_AI" = "false" ] && [ "$HAS_EMAIL" = "false" ]; then
            echo "⚠️  Neither AI nor Email configured, skipping this step"
            exit 0
          fi

          # 运行 AI 分析和邮件发送
          echo "Running AI analysis and email sender..."
          python -c "
import sys
import os
import json

sys.path.insert(0, 'src')

from ai_analyzer import AIAnalyzer
from email_sender import EmailSender

# 读取最新的分析结果
date_str = '${{ steps.get-date.outputs.date }}'
json_file = f'output/{date_str}.json'

if not os.path.exists(json_file):
    print(f'⚠️  Data file not found: {json_file}')
    sys.exit(0)

with open(json_file, 'r') as f:
    result = json.load(f)

data = result.get('data', [])
anomalies = result.get('anomalies', [])
summary = result.get('summary', {})

print(f'\n📊 Loaded data: {len(data)} tickers, {summary.get(\"total\", 0)} anomalies\n')

# AI 分析
ai_analyzer = AIAnalyzer()
email_sender = EmailSender()

analysis_text = None

if ai_analyzer.is_available():
    print('🤖 Running AI analysis...')
    analysis_text = ai_analyzer.analyze_market_data(data, anomalies, summary)
    if analysis_text:
        print('✓ AI analysis completed')
        print(f'\nAI Analysis Preview:\n{analysis_text[:200]}...\n')
    else:
        print('⚠️  AI analysis failed')
else:
    print('⊘ AI analysis not available (no API key)')
    analysis_text = '**AI 分析未配置**\n\n请配置 OPENAI_API_KEY 以启用 AI 智能分析功能。'

# 发送邮件
if email_sender.is_available():
    recipient = os.getenv('RECIPIENT_EMAIL', os.getenv('GMAIL_USER'))

    if recipient:
        print(f'\n📧 Sending email to {recipient}...')

        subject = ai_analyzer.generate_email_subject(data, summary.get('total', 0))
        html_content = ai_analyzer.format_for_email(analysis_text, data, summary)

        success = email_sender.send_report(recipient, subject, html_content)

        if success:
            print('✅ Email sent successfully!')
        else:
            print('❌ Failed to send email')
    else:
        print('⚠️  No recipient email configured')
else:
    print('⊘ Email not available (no Gmail credentials)')

print('\n✓ AI & Email step completed')
"

          echo ""
          echo "=================================================="

  # Job 2: 网页部署
  deploy:
    name: 🚀 Deploy to GitHub Pages
    needs: fetch-and-analyze
    runs-on: ubuntu-latest

    permissions:
      contents: write  # Needed for pushing to gh-pages

    steps:
      - name: 📥 Download analysis results
        uses: actions/download-artifact@v4
        with:
          name: analysis-results-${{ needs.fetch-and-analyze.outputs.analysis-date }}
          path: output

      - name: 🚀 Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./output
          publish_branch: gh-pages
          keep_files: true  # Keep historical data files
          commit_message: '📊 Update report - ${{ needs.fetch-and-analyze.outputs.analysis-date }}'
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          enable_jekyll: false

      - name: ✅ Deployment summary
        run: |
          echo "=================================================="
          echo "✅ DEPLOYMENT COMPLETE"
          echo "=================================================="
          echo "📅 Analysis Date: ${{ needs.fetch-and-analyze.outputs.analysis-date }}"
          echo "🔗 Report URL: https://onlinefchen.github.io/options-anomaly-detector/"
          echo "📦 Files deployed: $(ls output | wc -l)"
          echo "=================================================="
