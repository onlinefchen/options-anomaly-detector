name: Daily Options Analysis

on:
  # CSV-CENTRIC WORKFLOW: Run daily at 4:00 PM Beijing Time
  # Purpose: Check if Polygon has uploaded today's CSV file and process it
  # CSV is THE CORE prerequisite - without it, analysis cannot proceed
  schedule:
    - cron: '0 8 * * *'  # 08:00 UTC = 4:00 PM Beijing Time (daily)
                         # Runs once per day to check and process CSV

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      force_overwrite:
        description: 'ğŸ”„ å¼ºåˆ¶è¦†ç›– (Force Overwrite) - å³ä½¿æ•°æ®å·²å­˜åœ¨ä¹Ÿé‡æ–°ç”Ÿæˆ'
        required: false
        type: boolean
        default: false
      force_generate:
        description: 'ğŸ“œ å¼ºåˆ¶ç”Ÿæˆå†å²æ•°æ® (Historical Data Generation)'
        required: false
        type: boolean
        default: false
      start_date:
        description: 'ğŸ“… å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)'
        required: false
        type: string
        default: ''
      end_date:
        description: 'ğŸ“… ç»“æŸæ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºåˆ™åªç”Ÿæˆå¼€å§‹æ—¥æœŸ)'
        required: false
        type: string
        default: ''
      days_back:
        description: 'ğŸ“Š æˆ–è€…ï¼šç”Ÿæˆè¿‡å»Nä¸ªäº¤æ˜“æ—¥ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰'
        required: false
        type: number
        default: 0

  # Run on push to main for testing
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'main.py'
      - '.github/workflows/**'

jobs:
  # Job 1: æ•°æ®ä¸‹è½½ä¸åˆ†æ
  fetch-and-analyze:
    name: ğŸ“Š Data Fetching & Analysis
    runs-on: ubuntu-latest

    outputs:
      analysis-date: ${{ steps.get-date.outputs.date }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for historical analysis

      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ğŸ“¦ Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: ğŸ“… Get current date
        id: get-date
        run: echo "date=$(TZ='Asia/Shanghai' date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

      - name: ğŸ“‚ Restore historical data
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: gh-pages-data
        continue-on-error: true

      - name: ğŸ’¾ Copy historical JSON files
        run: |
          mkdir -p output
          if [ -d "gh-pages-data" ]; then
            echo "ğŸ“Š Copying historical JSON files for 10-day analysis..."
            find gh-pages-data -name "*.json" -type f -exec cp {} output/ \; || true
            echo "âœ“ Historical data restored: $(ls output/*.json 2>/dev/null | wc -l) files"
          else
            echo "âš ï¸  No historical data found (first run?)"
          fi

      - name: ğŸ” Check if data already exists
        id: check-data
        run: |
          echo "=================================================="
          echo "ğŸ” CHECKING IF DATA ALREADY EXISTS"
          echo "=================================================="

          CSV_DATE=$(TZ='Asia/Shanghai' python3 -c "from datetime import datetime; import sys; sys.path.insert(0, 'src'); from trading_calendar import get_previous_trading_day; print(get_previous_trading_day(datetime.now().strftime('%Y-%m-%d')))")
          echo "Target CSV date: $CSV_DATE"

          JSON_FILE="output/${CSV_DATE}.json"
          HTML_FILE="output/${CSV_DATE}.html"

          if [ -f "$JSON_FILE" ] && [ -f "$HTML_FILE" ]; then
            echo "âœ“ Data files found:"
            echo "  â€¢ JSON: $JSON_FILE"
            echo "  â€¢ HTML: $HTML_FILE"
            echo ""

            if [ "${{ github.event.inputs.force_overwrite }}" = "true" ]; then
              echo "ğŸ”„ Force overwrite enabled - will regenerate data"
              echo "skip_analysis=false" >> $GITHUB_OUTPUT
            else
              echo "âŠ˜ Data already exists and force_overwrite not set"
              echo "âŠ˜ Skipping analysis to avoid duplicate work"
              echo "skip_analysis=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "âŠ˜ Data files not found - proceeding with analysis"
            echo "skip_analysis=false" >> $GITHUB_OUTPUT
          fi

          echo "csv_date=$CSV_DATE" >> $GITHUB_OUTPUT
          echo "=================================================="

      - name: ğŸ“¡ CSV Availability Check
        if: steps.check-data.outputs.skip_analysis != 'true'
        run: |
          echo "=================================================="
          echo "ğŸ“¡ STEP 1: CSV AVAILABILITY CHECK"
          echo "=================================================="
          echo ""
          echo "âš ï¸  CSV FILE IS THE CORE PREREQUISITE"
          echo "   â†’ Without CSV, analysis cannot proceed"
          echo "   â†’ Workflow runs hourly (4-9 PM Beijing) until CSV is available"
          echo ""
          echo "Current time: $(TZ='Asia/Shanghai' date +'%Y-%m-%d %H:%M:%S %Z')"
          echo "Next retry: In 1 hour (if CSV not available)"
          echo ""
          echo "Checking Polygon Flat Files for today's CSV..."
          echo ""

      - name: ğŸ“œ Generate historical data (if requested)
        if: ${{ github.event.inputs.force_generate == 'true' && steps.check-data.outputs.skip_analysis != 'true' }}
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          POLYGON_S3_ACCESS_KEY: ${{ secrets.POLYGON_S3_ACCESS_KEY }}
          POLYGON_S3_SECRET_KEY: ${{ secrets.POLYGON_S3_SECRET_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          echo "=================================================="
          echo "ğŸ“œ GENERATING HISTORICAL DATA"
          echo "=================================================="
          echo ""

          # åˆ¤æ–­ä½¿ç”¨å“ªç§æ¨¡å¼
          if [ "${{ github.event.inputs.days_back }}" != "0" ] && [ "${{ github.event.inputs.days_back }}" != "" ]; then
            echo "æ¨¡å¼: ç”Ÿæˆè¿‡å» ${{ github.event.inputs.days_back }} ä¸ªäº¤æ˜“æ—¥"
            python generate_historical_data.py --days ${{ github.event.inputs.days_back }}
          elif [ -n "${{ github.event.inputs.start_date }}" ]; then
            if [ -n "${{ github.event.inputs.end_date }}" ]; then
              echo "æ¨¡å¼: ç”Ÿæˆæ—¥æœŸåŒºé—´ ${{ github.event.inputs.start_date }} è‡³ ${{ github.event.inputs.end_date }}"
              python generate_historical_data.py --start "${{ github.event.inputs.start_date }}" --end "${{ github.event.inputs.end_date }}"
            else
              echo "æ¨¡å¼: ç”Ÿæˆå•ä¸ªæ—¥æœŸ ${{ github.event.inputs.start_date }}"
              python generate_historical_data.py --date "${{ github.event.inputs.start_date }}"
            fi
          else
            echo "âŒ é”™è¯¯: å¿…é¡»æŒ‡å®š days_back æˆ– start_date"
            exit 1
          fi

          echo ""
          echo "âœ“ å†å²æ•°æ®ç”Ÿæˆå®Œæˆ"
          echo ""

      - name: ğŸ” Run anomaly detection
        if: steps.check-data.outputs.skip_analysis != 'true'
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          POLYGON_S3_ACCESS_KEY: ${{ secrets.POLYGON_S3_ACCESS_KEY }}
          POLYGON_S3_SECRET_KEY: ${{ secrets.POLYGON_S3_SECRET_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          FORCE_OVERWRITE: ${{ github.event.inputs.force_overwrite }}
        run: |
          echo "=================================================="
          echo "ğŸ” STEP 2: ANOMALY DETECTION & ANALYSIS"
          echo "=================================================="
          python main.py

      - name: ğŸ“š Generate archive index (always update)
        run: |
          echo "=================================================="
          echo "ğŸ“š GENERATING ARCHIVE INDEX"
          echo "=================================================="
          python3 -c "
          import sys
          sys.path.insert(0, 'src')
          from archive_index_generator import get_archived_reports, generate_archive_index
          reports = get_archived_reports('output')
          generate_archive_index(reports, 'output/archive.html')
          print(f'âœ“ Archive index generated: {len(reports)} reports')
          "
          echo ""

      - name: ğŸ“Š Upload analysis results
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results-${{ steps.get-date.outputs.date }}
          path: |
            output/
            !output/.gitkeep
          retention-days: 90

      - name: ğŸ¤– AI Analysis & Email Notification
        if: success() && github.event_name != 'push' && steps.check-data.outputs.skip_analysis != 'true'
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GMAIL_USER: ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASSWD: ${{ secrets.GMAIL_APP_PASSWD }}
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENT_EMAIL }}
        run: |
          echo "=================================================="
          echo "ğŸ¤– AI ANALYSIS & EMAIL NOTIFICATION"
          echo "=================================================="
          echo ""

          # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ–°æ•°æ®
          if [ ! -f "output/NEW_DATA_GENERATED" ]; then
            echo "âŠ˜ No new data generated (data already existed)"
            echo "   â†’ Skipping email notification to avoid duplicates"
            echo "=================================================="
            exit 0
          fi

          echo "âœ“ New data generated, proceeding with email notification"
          echo ""

          # æ£€æŸ¥æ˜¯å¦é…ç½®äº† AI æˆ–é‚®ä»¶
          HAS_AI=false
          HAS_EMAIL=false

          if [ -n "$OPENAI_API_KEY" ]; then
            echo "âœ“ OpenAI API Key configured"
            HAS_AI=true
          else
            echo "âŠ˜ OpenAI API Key not configured (skipping AI analysis)"
          fi

          if [ -n "$GMAIL_USER" ] && [ -n "$GMAIL_APP_PASSWD" ]; then
            echo "âœ“ Gmail credentials configured"
            HAS_EMAIL=true
          else
            echo "âŠ˜ Gmail not configured (skipping email)"
          fi

          echo ""

          # å¦‚æœä¸¤è€…éƒ½æœªé…ç½®ï¼Œè·³è¿‡
          if [ "$HAS_AI" = "false" ] && [ "$HAS_EMAIL" = "false" ]; then
            echo "âš ï¸  Neither AI nor Email configured, skipping this step"
            exit 0
          fi

          # è¿è¡Œ AI åˆ†æå’Œé‚®ä»¶å‘é€
          echo "Running AI analysis and email sender..."
          python scripts/run_ai_email.py

          echo ""
          echo "=================================================="

  # Job 2: ç½‘é¡µéƒ¨ç½²
  deploy:
    name: ğŸš€ Deploy to GitHub Pages
    needs: fetch-and-analyze
    if: success()
    runs-on: ubuntu-latest

    permissions:
      contents: write  # Needed for pushing to gh-pages

    steps:
      - name: ğŸ“¥ Download analysis results
        uses: actions/download-artifact@v4
        with:
          name: analysis-results-${{ needs.fetch-and-analyze.outputs.analysis-date }}
          path: output

      - name: ğŸš€ Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./output
          publish_branch: gh-pages
          keep_files: true  # Keep historical data files
          commit_message: 'ğŸ“Š Update report - ${{ needs.fetch-and-analyze.outputs.analysis-date }}'
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          enable_jekyll: false

      - name: âœ… Deployment summary
        run: |
          echo "=================================================="
          echo "âœ… DEPLOYMENT COMPLETE"
          echo "=================================================="
          echo "ğŸ“… Analysis Date: ${{ needs.fetch-and-analyze.outputs.analysis-date }}"
          echo "ğŸ”— Report URL: https://onlinefchen.github.io/options-anomaly-detector/"
          echo "ğŸ“¦ Files deployed: $(ls output | wc -l)"
          echo "=================================================="
