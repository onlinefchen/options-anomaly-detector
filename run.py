#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„ CLI å·¥å…· - å¤„ç†æ‰€æœ‰ workflow æ“ä½œ

Commands:
  daily-analysis     è¿è¡Œæ¯æ—¥åˆ†æ
  regenerate-html    é‡æ–°ç”Ÿæˆ HTML æŠ¥å‘Š
  test-email         æµ‹è¯•é‚®ä»¶å‘é€
  restore-data       ä» gh-pages æ¢å¤å†å²æ•°æ®
"""
import os
import sys
import json
import shutil
import argparse
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))


def restore_historical_data(source_dir: str, output_dir: str = 'output'):
    """
    ä» gh-pages åˆ†æ”¯æ¢å¤å†å² JSON æ–‡ä»¶

    Args:
        source_dir: gh-pages æ•°æ®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        æ¢å¤çš„æ–‡ä»¶æ•°é‡
    """
    print("ğŸ“‚ Restoring historical data...")
    os.makedirs(output_dir, exist_ok=True)

    if not os.path.exists(source_dir):
        print(f"âš ï¸  No historical data found (first run?)")
        return 0

    # Copy all JSON files
    count = 0
    for file in Path(source_dir).rglob('*.json'):
        if file.name.endswith('.json'):
            dest = os.path.join(output_dir, file.name)
            shutil.copy2(file, dest)
            count += 1

    print(f"âœ“ Historical data restored: {count} files")
    return count


def daily_analysis_command(args):
    """è¿è¡Œæ¯æ—¥åˆ†æ"""
    print("=" * 80)
    print("ğŸ“Š Daily Analysis")
    print("=" * 80)
    print()

    # Restore historical data if available
    if args.restore_from:
        restore_historical_data(args.restore_from)
        print()

    # æ ¹æ®å‚æ•°å†³å®šè¿è¡Œå“ªä¸ªè„šæœ¬
    if args.days_back and args.days_back > 0:
        # ç”Ÿæˆå†å²æ•°æ®
        print(f"ğŸ“Š Generating past {args.days_back} trading days")
        print()

        cmd_args = ['--days', str(args.days_back)]

        # Import and run
        import generate_historical_data
        sys.argv = ['generate_historical_data.py'] + cmd_args
        generate_historical_data.main()
    else:
        # è¿è¡Œæ­£å¸¸çš„æ¯æ—¥åˆ†æ
        print("ğŸ” Running daily analysis")
        print()

        import main
        exit_code = main.main()
        if exit_code != 0:
            sys.exit(exit_code)

    # ç”Ÿæˆ archive index
    print()
    print("ğŸ“š Generating archive index...")
    from archive_index_generator import get_archived_reports, generate_archive_index
    reports = get_archived_reports('output')
    generate_archive_index(reports, 'output/archive.html')
    print(f"âœ“ Archive index generated: {len(reports)} reports")

    print()
    print("=" * 80)
    print("âœ… Daily analysis complete!")
    print("=" * 80)


def regenerate_html_command(args):
    """é‡æ–°ç”Ÿæˆ HTML æŠ¥å‘Š"""
    from trading_calendar import get_trading_calendar

    print("=" * 80)
    print("ğŸ”„ Regenerate HTML Reports")
    print("=" * 80)
    print()

    # Restore data from gh-pages
    if args.restore_from:
        count = restore_historical_data(args.restore_from)
        if count == 0:
            print("âŒ No gh-pages data found")
            sys.exit(1)
        print()

    output_dir = args.output_dir or 'output'

    # æ”¶é›†éœ€è¦å¤„ç†çš„ JSON æ–‡ä»¶
    json_files = []

    if args.specific_date:
        # ç‰¹å®šæ—¥æœŸæ¨¡å¼
        print(f"Mode: Regenerate specific date {args.specific_date}")

        # Check if trading day
        calendar = get_trading_calendar()
        if not calendar.is_trading_day(args.specific_date):
            print(f"âŒ {args.specific_date} is not a trading day")
            print("   Only trading days can have valid data")
            sys.exit(1)

        json_file = os.path.join(output_dir, f'{args.specific_date}.json')
        if not os.path.exists(json_file):
            print(f"âŒ No JSON file found for {args.specific_date}")
            sys.exit(1)

        json_files = [json_file]
    else:
        # æœ€è¿‘ N å¤©æ¨¡å¼
        days = args.days or 7
        print(f"Mode: Regenerate last {days} days")
        print()

        # æŸ¥æ‰¾æ‰€æœ‰ JSON æ–‡ä»¶å¹¶æ’åº
        all_files = sorted(
            [f for f in os.listdir(output_dir)
             if f.endswith('.json') and len(f) == 15],  # YYYY-MM-DD.json
            reverse=True
        )[:days]

        if not all_files:
            print("âŒ No JSON files found")
            sys.exit(1)

        json_files = [os.path.join(output_dir, f) for f in all_files]

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    print(f"Processing {len(json_files)} file(s)...")
    print()

    success_count = 0
    calendar = get_trading_calendar()

    for idx, json_file in enumerate(json_files, 1):
        date = Path(json_file).stem
        print(f"[{idx}/{len(json_files)}] Processing {date}...")

        # Check if trading day
        if not calendar.is_trading_day(date):
            print(f"  âŠ˜ Skipped (not a trading day)")
            print()
            continue

        # Load JSON
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Generate HTML
            from report_generator import HTMLReportGenerator
            reporter = HTMLReportGenerator()
            html_file = json_file.replace('.json', '.html')

            reporter.generate(
                data=data.get('data', []),
                anomalies=data.get('anomalies', []),
                summary=data.get('summary', {}),
                metadata=data.get('metadata', {}),
                output_file=html_file
            )

            print(f"  âœ“ Success")
            success_count += 1
        except Exception as e:
            print(f"  âœ— Failed: {e}")

        print()

    print("=" * 80)
    print(f"Complete! Generated {success_count}/{len(json_files)} HTML files")
    print("=" * 80)
    print()

    # Regenerate archive index
    print("ğŸ“š Regenerating archive index...")
    from archive_index_generator import get_archived_reports, generate_archive_index
    reports = get_archived_reports(output_dir)
    generate_archive_index(reports, os.path.join(output_dir, 'archive.html'))
    print(f"âœ“ Archive index updated ({len(reports)} reports)")
    print()

    # Update index.html with latest
    html_files = sorted(
        [f for f in os.listdir(output_dir)
         if f.endswith('.html') and len(f) == 15],  # YYYY-MM-DD.html
        reverse=True
    )

    if html_files:
        latest_html = os.path.join(output_dir, html_files[0])
        index_html = os.path.join(output_dir, 'index.html')
        shutil.copy2(latest_html, index_html)
        date = Path(html_files[0]).stem
        print(f"âœ“ Copied latest report to index.html ({date})")
    else:
        print("âš ï¸  No HTML files found")

    print()
    print("=" * 80)
    print("âœ… HTML regeneration complete!")
    print("=" * 80)


def test_email_command(args):
    """æµ‹è¯•é‚®ä»¶å‘é€"""
    print("=" * 80)
    print("ğŸ“§ Testing Email Sending")
    print("=" * 80)
    print()

    gmail_user = os.getenv('GMAIL_USER')
    recipient = os.getenv('RECIPIENT_EMAIL')

    print(f"Gmail User: {gmail_user}")
    print(f"Recipient: {recipient}")
    print()

    # Run test_email.py
    import test_email
    test_email.main()

    print()
    print("=" * 80)
    print("âœ… Email test complete!")
    print("=" * 80)


def restore_data_command(args):
    """æ¢å¤å†å²æ•°æ®"""
    print("=" * 80)
    print("ğŸ“‚ Restore Historical Data")
    print("=" * 80)
    print()

    if not args.source:
        print("âŒ Error: --source is required")
        sys.exit(1)

    count = restore_historical_data(args.source, args.output_dir or 'output')

    print()
    print("=" * 80)
    print(f"âœ… Restored {count} files")
    print("=" * 80)


def main():
    load_dotenv()

    parser = argparse.ArgumentParser(
        description='ç»Ÿä¸€çš„ CLI å·¥å…· - å¤„ç†æ‰€æœ‰ workflow æ“ä½œ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # daily-analysis å‘½ä»¤
    daily_parser = subparsers.add_parser('daily-analysis', help='è¿è¡Œæ¯æ—¥åˆ†æ')
    daily_parser.add_argument('--days-back', type=int, default=0,
                            help='ç”Ÿæˆè¿‡å»Nä¸ªäº¤æ˜“æ—¥çš„æ•°æ®ï¼ˆ0=åªç”Ÿæˆä»Šå¤©ï¼‰')
    daily_parser.add_argument('--restore-from',
                            help='ä»æŒ‡å®šç›®å½•æ¢å¤å†å²æ•°æ®ï¼ˆå¦‚ gh-pages-dataï¼‰')

    # regenerate-html å‘½ä»¤
    regen_parser = subparsers.add_parser('regenerate-html', help='é‡æ–°ç”Ÿæˆ HTML æŠ¥å‘Š')
    regen_parser.add_argument('--days', type=int, default=7,
                            help='æ›´æ–°æœ€è¿‘Nå¤©çš„HTMLæŠ¥å‘Š')
    regen_parser.add_argument('--specific-date',
                            help='æŒ‡å®šç‰¹å®šæ—¥æœŸ (YYYY-MM-DD)')
    regen_parser.add_argument('--restore-from',
                            help='ä»æŒ‡å®šç›®å½•æ¢å¤å†å²æ•°æ®ï¼ˆå¦‚ gh-pages-dataï¼‰')
    regen_parser.add_argument('--output-dir', default='output',
                            help='è¾“å‡ºç›®å½•')

    # test-email å‘½ä»¤
    email_parser = subparsers.add_parser('test-email', help='æµ‹è¯•é‚®ä»¶å‘é€')

    # restore-data å‘½ä»¤
    restore_parser = subparsers.add_parser('restore-data', help='æ¢å¤å†å²æ•°æ®')
    restore_parser.add_argument('--source', required=True,
                              help='æºæ•°æ®ç›®å½•ï¼ˆå¦‚ gh-pages-dataï¼‰')
    restore_parser.add_argument('--output-dir', default='output',
                              help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # æ‰§è¡Œå¯¹åº”çš„å‘½ä»¤
    if args.command == 'daily-analysis':
        daily_analysis_command(args)
    elif args.command == 'regenerate-html':
        regenerate_html_command(args)
    elif args.command == 'test-email':
        test_email_command(args)
    elif args.command == 'restore-data':
        restore_data_command(args)


if __name__ == '__main__':
    main()
